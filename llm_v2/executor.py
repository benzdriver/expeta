from typing import Any, Callable, Optional, List
from langchain.text_splitter import TokenTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from llm_v2.client_factory import get_chat_client

async def run_prompt(
    *,
    user_message: Optional[str] = None,
    system_message: Optional[str] = None,
    messages: Optional[List[dict]] = None,
    model: str = "gpt-4",
    max_input_tokens: int = 8000,
    parse_response: Callable[[str], Any] = lambda x: x,
    merge_result: Callable[[Any, Any], Any] = lambda acc, x: x,
    get_system_prompt: Optional[Callable[[int, int], str]] = None,
    return_json: bool = False
) -> Any:
    """
    åŸºäº LangChain çš„ç»Ÿä¸€ LLM æ‰§è¡Œå™¨
    
    ç‰¹ç‚¹ï¼š
    1. è‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬åˆ†å—
    2. æ”¯æŒè‡ªå®šä¹‰å“åº”è§£æ
    3. æ”¯æŒç»“æœåˆå¹¶
    4. æ”¯æŒ JSON è¾“å‡º
    """
    # è·å– LLM å®¢æˆ·ç«¯
    llm = get_chat_client(model_name=model)
    
    # è®¾ç½®æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = TokenTextSplitter(
        model_name=model,
        chunk_size=max_input_tokens,
        chunk_overlap=200
    )
    
    # åˆ†å‰²è¾“å…¥æ–‡æœ¬
    if user_message:
        chunks = text_splitter.split_text(user_message)
    else:
        chunks = [""]  # ç©ºæ–‡æœ¬æƒ…å†µ
        
    print(f"ğŸ“„ æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
    
    # ä½¿ç”¨å­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨
    output_parser = StrOutputParser()
    
    # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
    accumulated_result = None
    for i, chunk in enumerate(chunks):
        try:
            # è·å–å½“å‰å—çš„ system prompt
            current_system = get_system_prompt(i + 1, len(chunks)) if get_system_prompt else system_message
            
            # åˆ›å»ºæç¤ºæ¨¡æ¿
            prompt = PromptTemplate.from_template(
                "{system}\n\n{user}" if current_system else "{user}"
            )
            
            # æ„å»ºé“¾
            chain = (
                prompt | 
                llm | 
                output_parser | 
                parse_response
            )
            
            # æ‰§è¡Œé“¾
            result = await chain.ainvoke({
                "system": current_system,
                "user": chunk
            })
            
            # åˆå¹¶ç»“æœ
            accumulated_result = merge_result(accumulated_result, result)
            
        except Exception as e:
            print(f"âŒ å¤„ç†å— {i+1}/{len(chunks)} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    return accumulated_result 