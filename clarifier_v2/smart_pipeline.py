print("=== pipeline å¯åŠ¨ ===")
from dotenv import load_dotenv
load_dotenv()
import asyncio
from pathlib import Path
from clarifier_v2.entity_discovery import discover_entities
from clarifier_v2.rag_retriever import (
    embedding_retrieve, 
    store_entity_summary, 
    retrieve_entity_summaries,
    retrieve_dependencies
)
from clarifier_v2.structured_summarizer import summarize_entity
from clarifier_v2.postprocess import recursive_refine, save_summary, processed_entities
from clarifier.schema.full_summary_schema import full_summary_schema
import json
import os

async def process_entity(entity, all_text, schema, output_path, processed=None, depth=0):
    """å¤„ç†å•ä¸ªå®ä½“å¹¶éªŒè¯ä¾èµ–å…³ç³»
    
    Args:
        entity: å®ä½“ä¿¡æ¯
        all_text: æ‰€æœ‰æ–‡æ¡£å†…å®¹
        schema: schemaæ ¼å¼
        output_path: è¾“å‡ºç›®å½•
        processed: å·²å¤„ç†çš„å®ä½“é›†åˆ
        depth: é€’å½’æ·±åº¦
    """
    if processed is None:
        processed = set()
    
    # æ„å»ºå®ä½“å”¯ä¸€æ ‡è¯†
    entity_key = f"{entity.get('name')}|{entity.get('type')}|{entity.get('parent')}"
    
    # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œå°±è·³è¿‡
    if entity_key in processed:
        print(f"âš ï¸ å®ä½“å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {entity['name']}")
        return
    
    # æ ‡è®°ä¸ºå·²å¤„ç†
    processed.add(entity_key)
    
    # è·å–ç›¸å…³ä¸Šä¸‹æ–‡
    print(f"ğŸ” è·å–å®ä½“ {entity['name']} çš„ä¸Šä¸‹æ–‡...")
    context = embedding_retrieve(entity["name"], all_text)
    
    # ç”Ÿæˆæ€»ç»“
    print(f"ğŸ“ ç”Ÿæˆç»“æ„åŒ–æ€»ç»“...")
    summary = await summarize_entity(entity, context, schema)
    
    # éªŒè¯ä¾èµ–å…³ç³»
    if "dependencies" in summary and summary["dependencies"]:
        print(f"ğŸ”„ éªŒè¯ä¾èµ–å…³ç³»...")
        dependencies = summary["dependencies"]
        valid_deps = []
        invalid_deps = []
        
        # è·å–ä¾èµ–é¡¹çš„æ‘˜è¦
        dep_summaries = retrieve_dependencies(dependencies)
        
        for dep in dependencies:
            if dep in dep_summaries:
                print(f"âœ… ä¾èµ–é¡¹æœ‰æ•ˆ: {dep}")
                valid_deps.append(dep)
            else:
                print(f"âš ï¸ ä¾èµ–é¡¹å¯èƒ½æ— æ•ˆ: {dep} (æœªæ‰¾åˆ°ç›¸å…³æ‘˜è¦)")
                # ä¹Ÿå¯ä»¥å°è¯•åœ¨åŸå§‹æ–‡æ¡£ä¸­æŸ¥æ‰¾
                dep_context = embedding_retrieve(dep, all_text)
                if dep and len(dep_context.strip()) > 100:  # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡
                    print(f"ğŸ“„ åœ¨åŸå§‹æ–‡æ¡£ä¸­æ‰¾åˆ°ä¾èµ–é¡¹ä¿¡æ¯: {dep}")
                    valid_deps.append(dep)
                else:
                    print(f"âŒ æ— æ³•éªŒè¯ä¾èµ–é¡¹: {dep}")
                    invalid_deps.append(dep)
        
        # æ›´æ–°ä¾èµ–åˆ—è¡¨ï¼Œåªä¿ç•™æœ‰æ•ˆçš„ä¾èµ–
        if invalid_deps:
            print(f"ğŸ§¹ ä»ä¾èµ–åˆ—è¡¨ä¸­ç§»é™¤æ— æ•ˆä¾èµ–: {invalid_deps}")
            summary["dependencies"] = valid_deps
    
    # ä¿å­˜æ€»ç»“
    save_summary(entity["name"], summary, output_path)
    
    # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    print(f"ğŸ“¥ å­˜å‚¨æ‘˜è¦åˆ°å‘é‡æ•°æ®åº“...")
    doc = store_entity_summary(entity["name"], summary)
    if doc:
        print(f"âœ… æ‘˜è¦å·²å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
    else:
        print(f"âŒ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“å¤±è´¥")
    
    # é€’å½’ç»†åŒ–
    print(f"ç»†åŒ–å®ä½“: {entity['name']}")
    await recursive_refine(entity, summary, all_text, schema, output_path, depth=0)

async def run_smart_pipeline(input_dir: str, output_dir: str):
    """è¿è¡Œæ™ºèƒ½ pipeline è¿›è¡Œæ–‡æ¡£åˆ†æå’Œæ€»ç»“
    
    Args:
        input_dir: è¾“å…¥æ–‡æ¡£ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
    """
    # æ¸…ç©ºå·²å¤„ç†å®ä½“é›†åˆï¼Œé˜²æ­¢å¤šæ¬¡è¿è¡Œæ—¶å‡ºç°é—®é¢˜
    processed_entities.clear()
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ“š è¯»å–æ–‡æ¡£...")
    all_text = ""
    for file in sorted(input_path.glob("*.md")):
        all_text += f"\n\n### FILE: {file.name} ###\n\n"
        all_text += file.read_text(encoding="utf-8")
    schema = json.dumps(full_summary_schema, indent=2)

    print("ğŸ” å‘ç°å®ä½“...")
    entities = await discover_entities(all_text)
    print(f"å‘ç°å®ä½“: {[e['name'] for e in entities]}")

    print("ğŸ“ ç”Ÿæˆç»“æ„åŒ–æ€»ç»“...")
    
    # æŒ‰å¤æ‚åº¦æ’åºå®ä½“ï¼Œä¼˜å…ˆå¤„ç†åŸºç¡€å®ä½“
    # 1. é¦–å…ˆå¤„ç†æ²¡æœ‰"/"çš„ç®€å•å®ä½“
    # 2. ç„¶åå¤„ç†åŒ…å«"/"çš„å¤æ‚å®ä½“ï¼ŒæŒ‰ç…§è·¯å¾„æ·±åº¦æ’åº
    simple_entities = [e for e in entities if '/' not in e.get('name', '')]
    complex_entities = [e for e in entities if '/' in e.get('name', '')]
    
    # æŒ‰åç§°é•¿åº¦/å¤æ‚åº¦æ’åº
    simple_entities.sort(key=lambda e: len(e.get('name', '')))
    complex_entities.sort(key=lambda e: len(e.get('name', '').split('/')))
    
    # åˆå¹¶æ’åºåçš„åˆ—è¡¨
    sorted_entities = simple_entities + complex_entities
    
    print(f"ğŸ”¢ å¤„ç†é¡ºåº:")
    for i, entity in enumerate(sorted_entities):
        print(f"  {i+1}. {entity.get('name')}")
    
    # é€ä¸ªå¤„ç†å®ä½“
    for entity in sorted_entities:
        # è·³è¿‡å·²å¤„ç†çš„å®ä½“
        entity_key = f"{entity.get('name')}|{entity.get('type')}|{entity.get('parent')}"
        if entity_key in processed_entities:
            print(f"âš ï¸ å®ä½“å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {entity['name']}")
            continue
            
        print(f"\nå¤„ç†å®ä½“: {entity['name']}")
        await process_entity(entity, all_text, schema, output_path, processed_entities)

    print("\nâœ… Pipeline å®Œæˆ!")
    print(f"è¾“å‡ºç›®å½•: {output_path}")
    print(f"å¤„ç†çš„å®ä½“æ€»æ•°: {len(processed_entities)}")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # è®¾ç½®é»˜è®¤è·¯å¾„
    input_dir = "data/input"
    output_dir = "data/output/v2/smart_modules"
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        input_dir = sys.argv[1]
    if len(sys.argv) > 2:
        output_dir = sys.argv[2]
    
    # è¿è¡Œ pipeline
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    asyncio.run(run_smart_pipeline(input_dir, output_dir))

if __name__ == "__main__":
    main() 