from llm.llm_executor import run_prompt as llm_run_prompt
from llm_v2.executor import run_prompt as llm_v2_run_prompt
from llm.chat_openai import chat
import tiktoken
import json
import re

tokenizer = tiktoken.encoding_for_model("gpt-4o")

SYSTEM_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æ¶æ„åˆ†æå·¥å…·ï¼Œä¸“é—¨ç”¨äºä»è½¯ä»¶è®¾è®¡æ–‡æ¡£ä¸­æå–å®ä½“ã€‚
ä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«æ‰€æœ‰APIç«¯ç‚¹ã€æœåŠ¡ã€ä»“å‚¨å±‚ã€å·¥å…·ç±»ã€æ•°æ®æ¨¡å‹ç­‰å…³é”®å®ä½“ã€‚
åªè¾“å‡ºJSONæ ¼å¼çš„å®ä½“åˆ—è¡¨ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–é™„åŠ å†…å®¹ã€‚'''

def get_entity_discovery_prompt(doc_text: str, chunk_index: int = None, total_chunks: int = None) -> str:
    chunk_info = ""
    if chunk_index is not None and total_chunks is not None:
        chunk_info = f"è¿™æ˜¯æ•´ä¸ªæ–‡æ¡£çš„ç¬¬{chunk_index}ä¸ªåˆ†å—ï¼Œå…±{total_chunks}ä¸ªåˆ†å—ã€‚ä»…åˆ†ææ­¤åˆ†å—ä¸­çš„å†…å®¹ã€‚\n\n"
    
    return f'''{chunk_info}åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œè¯†åˆ«æ‰€æœ‰è½¯ä»¶å®ä½“ï¼ˆAPIç«¯ç‚¹ã€æœåŠ¡ã€ä»“å‚¨å±‚ã€å·¥å…·ç±»ç­‰ï¼‰ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{doc_text}

è¾“å‡ºè¦æ±‚ï¼š
1. ä½¿ç”¨JSONæ•°ç»„æ ¼å¼
2. æ¯ä¸ªå®ä½“åŒ…å«nameï¼ˆåç§°ï¼‰ã€typeï¼ˆç±»å‹ï¼‰ã€parentï¼ˆæ‰€å±æ¨¡å—ï¼‰å­—æ®µ
3. ä½¿ç”¨code blockåŒ…è£¹JSON (```json)
4. ä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡æœ¬

ç¤ºä¾‹ï¼š
```json
[{"name": "auth/login", "type": "Function", "parent": "Auth"}]
```'''

def parse_entity_list(text: str):
    print("LLMåŸå§‹è¿”å›ï¼š", text)
    # å°è¯•æ‰¾åˆ°JSONéƒ¨åˆ†
    json_match = re.search(r'```json\n(.*?)\n```|```(.*?)```|\[.*\]', text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1) or json_match.group(2) or json_match.group(0)
        json_str = json_str.strip()
        print("æå–çš„JSONï¼š", json_str)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯ï¼š{e}")
            # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            print("è¿”å›ç©ºåˆ—è¡¨ä»¥ç»§ç»­å¤„ç†")
            return []
    else:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå†…å®¹ï¼Œè¿”å›ç©ºåˆ—è¡¨ä»¥ç»§ç»­å¤„ç†")
        return []

def merge_entities(accumulated_result, new_result):
    """åˆå¹¶ä¸¤ç»„å®ä½“åˆ—è¡¨ï¼Œç”¨äºåˆ†å—å¤„ç†"""
    if accumulated_result is None:
        return new_result
    if not new_result:
        return accumulated_result
    return accumulated_result + new_result

async def discover_entities(all_text: str):
    """
    ä»æ–‡æ¡£ä¸­å‘ç°æ‰€æœ‰å®ä½“ï¼Œæ”¯æŒå¤§å‹æ–‡æ¡£åˆ†å—å¤„ç†
    
    Args:
        all_text: æ–‡æ¡£å†…å®¹
    
    Returns:
        å®ä½“åˆ—è¡¨
    """
    print("ğŸ“š å¼€å§‹å®ä½“å‘ç°...")
    
    # ä½¿ç”¨llm_v2çš„run_promptè¿›è¡Œè‡ªåŠ¨åˆ†å—å¤„ç†
    entities = await llm_v2_run_prompt(
        user_message=all_text,
        system_message=SYSTEM_PROMPT,
        model="gpt-4o",
        max_input_tokens=4000,  # ä½¿ç”¨æ›´å¤§çš„åˆ†å—å¤§å°
        parse_response=parse_entity_list,
        merge_result=merge_entities,
        get_system_prompt=lambda chunk_idx, total_chunks: SYSTEM_PROMPT + f"\n\nè¿™æ˜¯æ–‡æ¡£çš„ç¬¬{chunk_idx}ä¸ªåˆ†å—ï¼Œå…±{total_chunks}ä¸ªåˆ†å—ã€‚ä»…åˆ†ææ­¤åˆ†å—ä¸­çš„å†…å®¹ã€‚"
    )
    
    # å»é‡å¤„ç†
    unique_entities = []
    seen = set()
    for entity in entities:
        entity_key = f"{entity.get('name')}|{entity.get('type')}|{entity.get('parent')}"
        if entity_key not in seen:
            seen.add(entity_key)
            unique_entities.append(entity)
    
    print(f"ğŸ” å®ä½“å‘ç°å®Œæˆ: åŸå§‹å®ä½“æ•°={len(entities)}, å»é‡å={len(unique_entities)}")
    return unique_entities 